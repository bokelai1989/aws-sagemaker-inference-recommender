{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to Use Inference Recommender \n",
    "\n",
    "Updated: July 23, 2022\n",
    "\n",
    "This notebook shows steps to run inference recommender for a registered model to help choose endpoint deployment instance type. Running this whole notebook using the current setting takes about 40 mins to finish end to end. \n",
    "\n",
    "* creating a model package group\n",
    "* registering a model package version to the package group with specific setting for inference recommender \n",
    "    - register a model package version is prerequisite to run inference recommender since there is some specific configuration needing to be done \n",
    "      when registering the model\n",
    "* use inferencer recommender to help choose instance size for the deployment\n",
    "\n",
    "Resources reference: \n",
    "\n",
    "[AWS sagemaker notebook example](https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-inference-recommender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdcution of Inference Recommender \n",
    "\n",
    "SageMaker Inference Recommender is a new capability of SageMaker that reduces the time required to get machine learning (ML) models in production by automating performance benchmarking and load testing models across SageMaker ML instances. You can use Inference Recommender to deploy your model to a real-time inference endpoint that delivers the best performance at the lowest cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.94.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sagemaker import get_execution_role, Session, session\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "sagemaker_session = Session()\n",
    "\n",
    "import sagemaker\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Package Group\n",
    "\n",
    "this section only needs to be executed once to create iniital registry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name = \"inference-recommender-model-registry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run below cell to create a new model package group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name = \"inference-recommender-model-registry\"\n",
    "model_package_group_description = \"testing for inference recommendor\"\n",
    "\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\": model_package_group_name,\n",
    "    \"ModelPackageGroupDescription\": model_package_group_description,\n",
    "}\n",
    "create_model_package_group_response = sm_client.create_model_package_group(\n",
    "    **model_package_group_input_dict\n",
    ")\n",
    "\n",
    "# create_mode_package_response = sm.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_model_package_group_response[\"ModelPackageGroupArn\"]\n",
    "print(\"ModelPackage Version ARN : {}\".format(model_package_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model URL for later model registry\n",
    "\n",
    "SageMaker models need to be packaged in .tar.gz files. When your SageMaker Endpoint is provisioned, the files in the archive will be extracted and put in /opt/ml/model/ on the Endpoint.\n",
    "\n",
    "To bring your own Deep Learning model, SageMaker expects a single archive file in .tar.gz format, containing a model file (*.pb) in TF SavedModel format and the script (*.py) for inference. (for example, use this command to create a tar file: !tar -cvpzf { \"model.tar.gz\"} ./model ./code)\n",
    "\n",
    "\n",
    "In this case, `model.tar.gz` file was generated from deploying model in `inference_testing.ipynb`. It is pakcaged file with a model.pth and code/inference.py (the ones that are used to deploy the endpoint). No need to change the inference.py. If debugging is needed, turn the log level to debug can be very helpful. \n",
    "\n",
    "This uri is taken from navigating in the SageMaker console to the endpoint, then the model, and then taking the S3 uri of the model artifact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"s3://sagemaker-us-east-2-*******/model/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a payload tar gz URL \n",
    "\n",
    "We need to create an archive that contains individual files that Inference Recommender can send to your Endpoint. Inference Recommender will randomly sample files from this archive so make sure it contains a similar distribution of payloads you'd expect in production. Note that your inference code must be able to read in the file formats from the sample payload.\n",
    "\n",
    "Please follow below steps to create a payload S3 url:\n",
    "\n",
    "1. create a folder \"sample-payload\" in local directory on SageMaker and put the testing images there\n",
    "    - <mark> for some reason, if using the same terminal command to compress tar gz the image folder locally on computer and upload it to S3 bucket, it does not work. It will give the error  \"INVALID_INPUT : 1. Inspect model request and try again.\" <mark>\n",
    "\n",
    "2. run below command to create a tar gz file\n",
    "\n",
    " !cd ./sample-payload/ && tar czvf ../payload.tar.gz * \n",
    "\n",
    "3. upload the tar.gz file to S3 bucket for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar czvf ../payload.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "payload_data_url = sagemaker_session.upload_data(path=\"payload_aid_images.tar.gz\", key_prefix=\"test\")\n",
    "print(\"model uploaded to: {}\".format(payload_data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model package tarball (model artifact + inference code)\n",
    "payload_data_url = sagemaker_session.upload_data(path=\"payload_aid_images.tar.gz\", key_prefix=\"test\")\n",
    "print(\"model uploaded to: {}\".format(payload_data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_payload_url = \"s3://\"+bucket+\"/\"+file_name_saved_in_s3  \n",
    "sample_payload_url= payload_data_url\n",
    "sample_payload_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register model in Model Registry\n",
    "\n",
    "In order to use Inference Recommender, you must have a versioned model in SageMaker Model Registry. To register a model in the Model Registry, you must have a model artifact packaged in a tarball and an inference container image. Registering a model includes the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ML model details for configuring creating model package \n",
    "\n",
    "Inference Recommender uses metadata about your ML model to recommend the best instance types and endpoint configurations for deployment. You can provide as much or as little information as you'd like but the more information you provide, the better your recommendations will be.\n",
    "\n",
    "ML Frameworks: TENSORFLOW, PYTORCH, XGBOOST, SAGEMAKER-SCIKIT-LEARN\n",
    "\n",
    "ML Domains: COMPUTER_VISION, NATURAL_LANGUAGE_PROCESSING, MACHINE_LEARNING\n",
    "\n",
    "Example ML Tasks: CLASSIFICATION, REGRESSION, IMAGE_CLASSIFICATION, OBJECT_DETECTION, SEGMENTATION, FILL_MASK, TEXT_CLASSIFICATION, TEXT_GENERATION, OTHER\n",
    "\n",
    "Note: Select the task that is the closest match to your model. Chose OTHER if none apply.\n",
    "\n",
    "In this step, you'll register your pretrained model that was packaged in the prior steps as a new version in SageMaker Model Registry. First, you'll configure the model package/version identifying which model package group this new model should be registered within as well as identify the initial approval status. You'll also identify the domain and task for your model. These values were set earlier in the notebook where ml_domain = 'COMPUTER_VISION' and ml_task = 'IMAGE_CLASSIFICATION'\n",
    "\n",
    "Note: ModelApprovalStatus is a configuration parameter that can be used in conjunction with SageMaker Projects to trigger automated deployment pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework = \"pytorch\"  # required for inference recommender job ;   \"tensorflow\" is another option \n",
    "framework_version = \"1.8.0\" #pytorch_framework_version\n",
    "model_name = \"deeplearning-model\" \n",
    "\n",
    "instance_type = \"ml.c4.xlarge\" \n",
    "\n",
    "# ML model details\n",
    "ml_domain = \"COMPUTER_VISION\"  # required for inference recommender job\n",
    "ml_task = \"IMAGE_SEGMENTATION\"\n",
    "\n",
    "model_package_description = \"{} {} inference recommender\".format(framework, model_name)\n",
    "\n",
    "model_approval_status = \"PendingManualApproval\"\n",
    "\n",
    "# define the parameters for creating model package \n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\": model_package_group_name,\n",
    "    \"Domain\": ml_domain.upper(),\n",
    "    \"Task\": ml_task.upper(),\n",
    "    \"SamplePayloadUrl\": sample_payload_url,\n",
    "    \"ModelPackageDescription\": model_package_description,\n",
    "    \"ModelApprovalStatus\": model_approval_status,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-inference:1.8.0-cpu-py36\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"1.8.0\",\n",
    "    py_version=\"py36\",\n",
    "    image_scope='inference',\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up inference specification\n",
    "\n",
    "You'll now setup the inference specification configuration for your model version. This contains information on how the model should be hosted.\n",
    "\n",
    "Inference Recommender expects a single input MIME type for sending requests. Learn more about [common inference data formats](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html) on SageMaker. This MIME type will be sent in the Content-Type header when invoking your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mime_types = [\"application/x-image\"]\n",
    "\n",
    "# user provides desired instance for testing comparison; if not provided, inference recommender will automatically tune \n",
    "supported_realtime_inference_types = [\"ml.c4.xlarge\", \"ml.c5.xlarge\", \"ml.m5.xlarge\", \"ml.c5d.large\", \"ml.m5.large\", \"ml.inf1.xlarge\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_input_configuration = '{\"input_1\":[1,224,224,3]}' --# this is for Optional: Model optimization using SageMaker Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify more configurations for model package inference\n",
    "\n",
    "modelpackage_inference_specification =  {\n",
    "    \"InferenceSpecification\": {\n",
    "      \"Containers\": [\n",
    "         {\n",
    "            \"Image\": image_uri,\n",
    "            \"ModelDataUrl\": model_url,\n",
    "            \"Framework\": framework.upper(),  # required \n",
    "            \"FrameworkVersion\": framework_version,\n",
    "            \"NearestModelName\": model_name\n",
    "#              \"ModelInput\": {\"DataInputConfig\": data_input_configuration},\n",
    "         }\n",
    "      ],\n",
    "      \"SupportedContentTypes\": input_mime_types,  # required, must be non-null # application/x-image\n",
    "      \"SupportedResponseMIMETypes\": [],\n",
    "      \"SupportedRealtimeInferenceInstanceTypes\": supported_realtime_inference_types,  # optional\n",
    "   }\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-488955376385/credential-segmenter-ny-test/model.tar.gz'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\n",
    "    \"ModelDataUrl\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the create_model_package_input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_package_input_dict.update(modelpackage_inference_specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageGroupName': 'inference-recommender-model-registry',\n",
       " 'Domain': 'COMPUTER_VISION',\n",
       " 'Task': 'IMAGE_SEGMENTATION',\n",
       " 'SamplePayloadUrl': 's3://sagemaker-us-east-2-488955376385/test/payload_aid_images.tar.gz',\n",
       " 'ModelPackageDescription': 'pytorch credentialSegmenter inference recommender',\n",
       " 'ModelApprovalStatus': 'PendingManualApproval',\n",
       " 'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-inference:1.8.0-cpu-py36',\n",
       "    'ModelDataUrl': 's3://sagemaker-us-east-2-488955376385/credential-segmenter-ny-test/model.tar.gz',\n",
       "    'Framework': 'PYTORCH',\n",
       "    'FrameworkVersion': '1.8.0',\n",
       "    'NearestModelName': 'credentialSegmenter'}],\n",
       "  'SupportedContentTypes': ['application/x-image'],\n",
       "  'SupportedResponseMIMETypes': [],\n",
       "  'SupportedRealtimeInferenceInstanceTypes': ['ml.c5d.large',\n",
       "   'ml.inf1.xlarge',\n",
       "   'ml.m5.xlarge']}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_package_input_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPackage Version ARN : arn:aws:sagemaker:us-east-2:488955376385:model-package/inference-recommender-model-registry/33\n"
     ]
    }
   ],
   "source": [
    "create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "\n",
    "print('ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker Inference Recommender Default Job\n",
    "\n",
    "Now with your model in Model Registry, you can kick off a 'Default' job to get instance recommendations. This only requires your ModelPackageVersionArn and comes back with recommendations within an hour.\n",
    "\n",
    "The output is a list of instance type recommendations with associated environment variables, cost, throughput and latency metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_name: credentialSegmenter-instance-1655912131\n",
      "job_description: pytorch credentialSegmenter\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "default_job_name = model_name + \"-instance-\" + str(round(time.time()))\n",
    "job_description = \"{} {}\".format(framework, model_name)\n",
    "job_type = \"Default\"\n",
    "\n",
    "rv = sm_client.create_inference_recommendations_job(\n",
    "    JobName=default_job_name,\n",
    "    JobDescription=job_description,  # optional\n",
    "    JobType=job_type,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\"ModelPackageVersionArn\": model_package_arn},\n",
    ")\n",
    "\n",
    "\n",
    "print(\"job_name:\", default_job_name)\n",
    "print(\"job_description:\", job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobArn': 'arn:aws:sagemaker:us-east-2:488955376385:inference-recommendations-job/credentialsegmenter-instance-1655912131', 'ResponseMetadata': {'RequestId': 'a75518b6-927a-49ee-ada6-bc257e672db4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'a75518b6-927a-49ee-ada6-bc257e672db4', 'content-type': 'application/x-amz-json-1.1', 'content-length': '123', 'date': 'Wed, 22 Jun 2022 15:35:31 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "rv = sm_client.create_inference_recommendations_job(\n",
    "    JobName=default_job_name,\n",
    "    JobDescription=job_description,  # optional\n",
    "    JobType=job_type,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\"ModelPackageVersionArn\": model_package_arn},\n",
    ")\n",
    "\n",
    "print(rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Instance Recommendation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Method 1 is to view the results under the console of SageMaker Inference Recommender and click the corresponding job name; you can also directly deploy an endpoint from the chosen instance there on the console\n",
    "\n",
    "- Method 2 is to run below code to extract the results here. \n",
    "\n",
    "Each inference recommendation includes `InstanceType`, `InitialInstanceCount`, `EnvironmentParameters` which are tuned environment variable parameters for better performance. We also include performance and cost metrics such as `MaxInvocations`, `ModelLatency`, `CostPerHour` and `CostPerInference`. We believe these metrics will help you narrow down to a specific endpoint configuration that suits your use case. \n",
    "\n",
    "Example:   \n",
    "\n",
    "If your motivation is overall price-performance with an emphasis on throughput, then you should focus on `CostPerInference` metrics  \n",
    "If your motivation is a balance between latency and throughput, then you should focus on `ModelLatency` / `MaxInvocations` metrics\n",
    "\n",
    "| Metric | Description |\n",
    "| --- | --- |\n",
    "| ModelLatency | The interval of time taken by a model to respond as viewed from SageMaker. This interval includes the local communication times taken to send the request and to fetch the response from the container of a model and the time taken to complete the inference in the container. <br /> Units: Milliseconds |\n",
    "| MaximumInvocations | The maximum number of InvokeEndpoint requests sent to an endpoint per minute. <br /> Units: None |\n",
    "| CostPerHour | The estimated cost per hour for your real-time endpoint. <br /> Units: US Dollars |\n",
    "| CostPerInference | The estimated cost per inference for your real-time endpoint. <br /> Units: US Dollars |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "Inference recommender job completed\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "finished = False\n",
    "while not finished:\n",
    "    inference_recommender_job = sm_client.describe_inference_recommendations_job(\n",
    "        JobName=str(default_job_name)\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        finished = True\n",
    "    else:\n",
    "        print(\"In progress\")\n",
    "        time.sleep(300)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Inference recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".format(inference_recommender_job[\"FailureReason\"]))\n",
    "else:\n",
    "    print(\"Inference recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailing out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>InstanceType</th>\n",
       "      <th>InitialInstanceCount</th>\n",
       "      <th>EnvironmentParameters</th>\n",
       "      <th>CostPerHour</th>\n",
       "      <th>CostPerInference</th>\n",
       "      <th>MaxInvocations</th>\n",
       "      <th>ModelLatency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sm-epc-9ac63c3e-d15e-4518-8b37-96f95fc41e53</td>\n",
       "      <td>ml.m5.large</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>487</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sm-epc-232d401e-ba3e-4154-8706-2f447843c7d4</td>\n",
       "      <td>ml.c5d.large</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>532</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sm-epc-f1d0b548-ec89-47e2-9252-a794b94764c2</td>\n",
       "      <td>ml.c4.xlarge</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>609</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sm-epc-0d3d57a6-59ec-4300-b4f3-5a6c200c8a4c</td>\n",
       "      <td>ml.c5.xlarge</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>757</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  EndpointName  InstanceType  \\\n",
       "0  sm-epc-9ac63c3e-d15e-4518-8b37-96f95fc41e53   ml.m5.large   \n",
       "1  sm-epc-232d401e-ba3e-4154-8706-2f447843c7d4  ml.c5d.large   \n",
       "2  sm-epc-f1d0b548-ec89-47e2-9252-a794b94764c2  ml.c4.xlarge   \n",
       "3  sm-epc-0d3d57a6-59ec-4300-b4f3-5a6c200c8a4c  ml.c5.xlarge   \n",
       "\n",
       "   InitialInstanceCount EnvironmentParameters  CostPerHour  CostPerInference  \\\n",
       "0                     1                    []        0.115          0.000004   \n",
       "1                     1                    []        0.115          0.000004   \n",
       "2                     1                    []        0.239          0.000007   \n",
       "3                     1                    []        0.204          0.000004   \n",
       "\n",
       "   MaxInvocations  ModelLatency  \n",
       "0             487           604  \n",
       "1             532           371  \n",
       "2             609           309  \n",
       "3             757           389  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {**x[\"EndpointConfiguration\"], **x[\"ModelConfiguration\"], **x[\"Metrics\"]}\n",
    "    for x in inference_recommender_job[\"InferenceRecommendations\"]\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(\"VariantName\", inplace=True, axis=1)\n",
    "pd.set_option(\"max_colwidth\", 400)\n",
    "df.head()\n",
    "\n",
    "# supported_realtime_inference_types = [\"ml.c4.xlarge\", \"ml.c5.xlarge\", \"ml.m5.xlarge\", \"ml.c5d.large\", \"ml.m5.large\", \"ml.inf1.xlarge\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Load Test\n",
    "\n",
    "With an 'Advanced' job, you can provide your production requirements, select instance types, tune environment variables and perform more extensive load tests. This typically takes 2 hours depending on your traffic pattern and number of instance types.\n",
    "\n",
    "The output is a list of endpoint configuration recommendations (instance type, instance count, environment variables) with associated cost, throughput and latency metrics.\n",
    "\n",
    "In the below example, we are tuning the endpoint against an environment variable OMP_NUM_THREADS with values [1, 2, 4] and we aim to limit the latency requirement to 500 ms. The goal is to find which value for OMP_NUM_THREADS provides the best performance.\n",
    "\n",
    "For some context, Python internally uses OpenMP for implementing multithreading within processes. The default value for OMP_NUM_THREADS is equal to the number of CPU core. However, when implemented on top of Simultaneous Multi Threading (SMT) such Intelâ€™s HypeThreading, a certain process might oversubscribe a particular core by spawning twice the threads as the number of actual CPU cores. In certain cases, a Python binary might end up spawning up to four times the threads as available actual processor cores. Therefore, an ideal setting for this parameter, if you have oversubscribed available cores using worker threads, is 1 or half the number of CPU cores on a SMT-enabled CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.c5.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobArn': 'arn:aws:sagemaker:us-east-2:488955376385:inference-recommendations-job/bab991c6-f1a8-11ec-b976-c1f8852c95fc', 'ResponseMetadata': {'RequestId': 'b5a59c11-acdb-478c-a1fe-e3a9606202c6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b5a59c11-acdb-478c-a1fe-e3a9606202c6', 'content-type': 'application/x-amz-json-1.1', 'content-length': '120', 'date': 'Tue, 21 Jun 2022 21:25:53 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "advanced_job = uuid.uuid1()\n",
    "advanced_response = sm_client.create_inference_recommendations_job(\n",
    "    JobName=str(advanced_job),\n",
    "    JobDescription=\"\",\n",
    "    JobType=\"Advanced\",\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        \"ModelPackageVersionArn\": model_package_arn,\n",
    "        \"JobDurationInSeconds\": 7200,\n",
    "        \"EndpointConfigurations\": [\n",
    "            {\n",
    "                \"InstanceType\": instance_type,\n",
    "                \"EnvironmentParameterRanges\": {\n",
    "                    \"CategoricalParameterRanges\": [\n",
    "                        {\"Name\": \"OMP_NUM_THREADS\", \"Value\": [\"1\", \"2\", \"4\"]}\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"ResourceLimit\": {\"MaxNumberOfTests\": 3, \"MaxParallelOfTests\": 1},\n",
    "        \"TrafficPattern\": {\n",
    "            \"TrafficType\": \"PHASES\",\n",
    "            \"Phases\": [{\"InitialNumberOfUsers\": 1, \"SpawnRate\": 1, \"DurationInSeconds\": 120}],\n",
    "        },\n",
    "    },\n",
    "    StoppingConditions={\n",
    "        \"MaxInvocations\": 1000,\n",
    "        \"ModelLatencyThresholds\": [{\"Percentile\": \"P95\", \"ValueInMilliseconds\": 500}],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(advanced_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Load Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "Inference recommender job completed\n"
     ]
    }
   ],
   "source": [
    "finished = False\n",
    "while not finished:\n",
    "    inference_recommender_job = sm_client.describe_inference_recommendations_job(\n",
    "        JobName=str(advanced_job)\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        finished = True\n",
    "    else:\n",
    "        print(\"In progress\")\n",
    "        time.sleep(300)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Inference recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".format(inference_recommender_job[\"FailureReason\"]))\n",
    "else:\n",
    "    print(\"Inference recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailing out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>InstanceType</th>\n",
       "      <th>InitialInstanceCount</th>\n",
       "      <th>EnvironmentParameters</th>\n",
       "      <th>CostPerHour</th>\n",
       "      <th>CostPerInference</th>\n",
       "      <th>MaxInvocations</th>\n",
       "      <th>ModelLatency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sm-epc-42aee4fd-5240-416c-8548-bd0ff6150b14</td>\n",
       "      <td>ml.c5.xlarge</td>\n",
       "      <td>4</td>\n",
       "      <td>[{'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1320</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sm-epc-d50b9303-c12d-4927-b548-08f4efac1a31</td>\n",
       "      <td>ml.c5.xlarge</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '2'}]</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1132</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sm-epc-0e2dedac-fc1c-468d-b5f7-e1b5ced3e2ea</td>\n",
       "      <td>ml.c5.xlarge</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '4'}]</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1098</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  EndpointName  InstanceType  \\\n",
       "0  sm-epc-42aee4fd-5240-416c-8548-bd0ff6150b14  ml.c5.xlarge   \n",
       "1  sm-epc-d50b9303-c12d-4927-b548-08f4efac1a31  ml.c5.xlarge   \n",
       "2  sm-epc-0e2dedac-fc1c-468d-b5f7-e1b5ced3e2ea  ml.c5.xlarge   \n",
       "\n",
       "   InitialInstanceCount  \\\n",
       "0                     4   \n",
       "1                     2   \n",
       "2                     2   \n",
       "\n",
       "                                               EnvironmentParameters  \\\n",
       "0  [{'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '1'}]   \n",
       "1  [{'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '2'}]   \n",
       "2  [{'Key': 'OMP_NUM_THREADS', 'ValueType': 'string', 'Value': '4'}]   \n",
       "\n",
       "   CostPerHour  CostPerInference  MaxInvocations  ModelLatency  \n",
       "0        0.816          0.000010            1320           119  \n",
       "1        0.408          0.000006            1132            94  \n",
       "2        0.408          0.000006            1098            97  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {**x[\"EndpointConfiguration\"], **x[\"ModelConfiguration\"], **x[\"Metrics\"]}\n",
    "    for x in inference_recommender_job[\"InferenceRecommendations\"]\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(\"VariantName\", inplace=True, axis=1)\n",
    "pd.set_option(\"max_colwidth\", 400)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (MXNet 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/mxnet-1.6-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
